{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes\n",
    "- why numpy==1.16.4 is used and not the most recent: https://github.com/tensorflow/tensorflow/issues/31249"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to start with a simple experiment, which shows that the concept is working at all.\n",
    "\n",
    "Target is to train a model to differentiate links and bases from non-fitting drawings. For this task a dataset consisting of about 500 examples each was created.\n",
    "I want to go through *n* TODO different steps to show, that the model can differentiate *non-links* from *links*. Then the same algortithm is used to determine *bases*.\n",
    "\n",
    "This is the first try in a series of steps taken to create a neural network to identify fourbar linkages from sketches and map them to their digital counterparts.\n",
    "\n",
    "The steps taken are as follows:\n",
    " 1. Acquire data from local harddrive (and then show loaded images).\n",
    " 2. Prepare data by creating tensors of image, label pairs.\n",
    " 3. Create a simple CNN to classify \"links\" from \"non-hits\" (*o*'s from *n*'s).\n",
    " 4. Train model.\n",
    " 5. Evaluate results.\n",
    " \n",
    "After these four steps the model should be trained with a variety of hyperparameters to see which is the most promising one.\n",
    "\n",
    "-> Further steps will try to use these models inside another CNN to get the coordinate of hits in a sketch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "Acquire Data and put them in proper directories to train on them.\n",
    "\n",
    "Data is stored in ../data/{n, o, x} with either \"no match\", \"joints\" or \"bases\" respectively.\n",
    "It is not in the working directory, because multiple approaches (with different programming languages) are sought to be used on this dataset.\n",
    "\n",
    "At first the right environment is created inside the working directory\n",
    "\n",
    "- data\n",
    "    - train\n",
    "        - n\n",
    "        - o\n",
    "        - x\n",
    "    - validate\n",
    "        - n\n",
    "        - o\n",
    "        - x\n",
    "    - test\n",
    "        - n\n",
    "        - o\n",
    "        - x\n",
    "        \n",
    "In these folders a subset of the *linkages* or *bases* and *non-hits* are placed to be used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, exists\n",
    "from os import mkdir\n",
    "\n",
    "def mkdir_ex(path):\n",
    "    if not exists(path):\n",
    "        mkdir(path)\n",
    "\n",
    "n_dir = join('..', 'data', 'n') # non hits\n",
    "o_dir = join('..', 'data', 'o') # links\n",
    "x_dir = join('..', 'data', 'x') # bases\n",
    "\n",
    "data = join('.', 'data')\n",
    "mkdir_ex(data)\n",
    "# Create bases directories for training, validation and testing\n",
    "train_dir = join(data, 'train')\n",
    "mkdir_ex(train_dir)\n",
    "validation_dir = join(data, 'validation')\n",
    "mkdir_ex(validation_dir)\n",
    "test_dir = join(data, 'test')\n",
    "mkdir_ex(test_dir)\n",
    "# Create respective training directories for data\n",
    "train_nohit = join(train_dir, 'n')\n",
    "mkdir_ex(train_nohit)\n",
    "train_links = join(train_dir, 'o')\n",
    "mkdir_ex(train_links)\n",
    "train_bases = join(train_dir, 'x')\n",
    "mkdir_ex(train_bases)\n",
    "# And validation directories\n",
    "validate_nohit = join(validation_dir, 'n')\n",
    "mkdir_ex(validate_nohit)\n",
    "validate_links = join(validation_dir, 'o')\n",
    "mkdir_ex(validate_links)\n",
    "validate_bases = join(validation_dir, 'x')\n",
    "mkdir_ex(validate_bases)\n",
    "# And test directories\n",
    "test_nohit = join(test_dir, 'n')\n",
    "mkdir_ex(test_nohit)\n",
    "test_links = join(test_dir, 'o')\n",
    "mkdir_ex(test_links)\n",
    "test_bases = join(test_dir, 'x')\n",
    "mkdir_ex(test_bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all folders are created and ready to be filled, the data is now propagated to their directories.\n",
    "\n",
    "The dataset consists of at least 500 entries each.\n",
    "To be exact, we take 500 images and distribute them about 60/20/20 into training, validation and test. This means each set brings:\n",
    "300 entries into training.\n",
    "100 entries into validation.\n",
    "100 entries into test.\n",
    "\n",
    "Another helpful aspect is, that the original data stays untouched and can not be compromised in any way.\n",
    "\n",
    "To increase the number of data via augmentation is a subject of later debate, if there is improvement to be expected.\n",
    "\n",
    "Since all data is named {0,1,2,3,4,5...}.jpeg inside their labelset, we can use this property to easily distribute the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def distribute_data(target_dir, src_dir, begin, limit):\n",
    "    for i in range(begin, limit):\n",
    "        filename = str(i) + '.jpeg'\n",
    "        src = join(src_dir, filename)\n",
    "        target = join(target_dir, filename)\n",
    "        copyfile(src, target)\n",
    "\n",
    "distribute_data(train_nohit, n_dir, 0, 300)\n",
    "distribute_data(train_links, o_dir, 0, 300)\n",
    "distribute_data(train_bases, x_dir, 0, 300)\n",
    "distribute_data(validate_nohit, n_dir, 300, 400)\n",
    "distribute_data(validate_links, o_dir, 300, 400)\n",
    "distribute_data(validate_bases, x_dir, 300, 400)\n",
    "distribute_data(test_nohit, n_dir, 400, 500)\n",
    "distribute_data(test_links, o_dir, 400, 500)\n",
    "distribute_data(test_links, x_dir, 400, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Preprocess data to be fit to be used. (Maybe data preprocessing is better to be done after model definition, because the model determines the input shape).\n",
    "\n",
    "The data has to be transformed into tensors which can be fed into the model.\n",
    "Four steps are suggested by the book (p.135):\n",
    " - Read the picture files.\n",
    " - Decode the JPEG content to RGB grids of pixels.\n",
    " - Convert these into floating-point tensors.\n",
    " - Rescale the pixel values (between 0 and 255) to the [0, 1] interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 900 images belonging to 3 classes.\n",
      "Found 300 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(512, 512),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(512, 512),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Now a generic model for testing is created.\n",
    "\n",
    "Here the model from deep learning with python p. 134 is used.\n",
    "\n",
    "This should be reduced later an analyzed on my own. But for a quick proof of concept this should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 510, 510, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 255, 255, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 253, 253, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 126, 126, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 124, 124, 128)     73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 62, 62, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 60, 60, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 115200)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               58982912  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 59,224,257\n",
      "Trainable params: 59,224,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(512, 512, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model should be configured for training.\n",
    "Therefore optimizers are imported. For binary classification the loss function 'binary_crossentropy' and as optimizer 'RMSprop' is used.\n",
    "\n",
    "\n",
    "This is recommended by Francois Chollet. Why this is the case is a matter of further research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=1e-4), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "Training the model is done via the \"fit\" method. for this the train_generator has to be used.\n",
    "\n",
    "## Disclaimer: At the moment train_generator has 3 classes. I have to select 2 of them!\n",
    "\n",
    "Tensorboard should be used for visualisation. Therefore a log directory is created, with a suitable callback object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From c:\\users\\me\\devel\\srp\\deepmech\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "100/100 [==============================] - 1772s 18s/step - loss: -2.7300 - acc: 0.4430 - val_loss: -3.2941 - val_acc: 0.5030\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 1785s 18s/step - loss: -3.9927 - acc: 0.5480 - val_loss: -3.3731 - val_acc: 0.5300\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 1820s 18s/step - loss: -4.4085 - acc: 0.5840 - val_loss: -3.3816 - val_acc: 0.5330\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 1046s 10s/step - loss: -4.6053 - acc: 0.6120 - val_loss: -3.3854 - val_acc: 0.5540\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 1037s 10s/step - loss: -4.5279 - acc: 0.6305 - val_loss: -3.4670 - val_acc: 0.5560\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 1037s 10s/step - loss: -4.7028 - acc: 0.6375 - val_loss: -3.4905 - val_acc: 0.5570\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 997s 10s/step - loss: -4.8185 - acc: 0.6310 - val_loss: -3.6006 - val_acc: 0.5610\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 976s 10s/step - loss: -4.8181 - acc: 0.6390 - val_loss: -3.4949 - val_acc: 0.5630\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 986s 10s/step - loss: -4.7663 - acc: 0.6485 - val_loss: -3.3641 - val_acc: 0.5330\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 979s 10s/step - loss: -4.8443 - acc: 0.6490 - val_loss: -3.4617 - val_acc: 0.5660\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 966s 10s/step - loss: -4.8861 - acc: 0.6410 - val_loss: -3.4178 - val_acc: 0.5690\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 966s 10s/step - loss: -4.7328 - acc: 0.6575 - val_loss: -3.5260 - val_acc: 0.5410\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 965s 10s/step - loss: -4.9475 - acc: 0.6410 - val_loss: -2.9451 - val_acc: 0.5100\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 967s 10s/step - loss: -4.8120 - acc: 0.6515 - val_loss: -3.2259 - val_acc: 0.5740\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 966s 10s/step - loss: -4.8725 - acc: 0.6495 - val_loss: -3.2373 - val_acc: 0.5790\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 969s 10s/step - loss: -4.7936 - acc: 0.6550 - val_loss: -3.2997 - val_acc: 0.5600\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 966s 10s/step - loss: -4.9525 - acc: 0.6470 - val_loss: -3.2018 - val_acc: 0.5660\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 965s 10s/step - loss: -4.8073 - acc: 0.6540 - val_loss: -3.1733 - val_acc: 0.5730\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 966s 10s/step - loss: -4.8453 - acc: 0.6540 - val_loss: -3.1494 - val_acc: 0.5810\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 966s 10s/step - loss: -4.9009 - acc: 0.6500 - val_loss: -3.1685 - val_acc: 0.5800\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 967s 10s/step - loss: -4.9052 - acc: 0.6485 - val_loss: -3.4020 - val_acc: 0.5710\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 986s 10s/step - loss: -4.9219 - acc: 0.6520 - val_loss: -3.3843 - val_acc: 0.5830\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 973s 10s/step - loss: -4.8606 - acc: 0.6530 - val_loss: -3.1164 - val_acc: 0.5820\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 967s 10s/step - loss: -4.8019 - acc: 0.6550 - val_loss: -3.2939 - val_acc: 0.5530\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 966s 10s/step - loss: -4.8223 - acc: 0.6555 - val_loss: -3.2428 - val_acc: 0.5670\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 966s 10s/step - loss: -5.0738 - acc: 0.6395 - val_loss: -3.1900 - val_acc: 0.5710\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 966s 10s/step - loss: -4.8070 - acc: 0.6575 - val_loss: -3.2652 - val_acc: 0.5700\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 966s 10s/step - loss: -4.9143 - acc: 0.6515 - val_loss: -3.2397 - val_acc: 0.5680\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 967s 10s/step - loss: -4.7773 - acc: 0.6505 - val_loss: -3.4742 - val_acc: 0.5590\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 967s 10s/step - loss: -4.8326 - acc: 0.6570 - val_loss: -3.1211 - val_acc: 0.5810\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "log_dir=(join('.', 'logs'))\n",
    "mkdir_ex(log_dir)\n",
    "\n",
    "callbacks = [ TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    embeddings_freq=1) ]\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
