{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes\n",
    "- why numpy==1.16.4 is used and not the most recent: https://github.com/tensorflow/tensorflow/issues/31249"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to start with a simple experiment, which shows that the concept is working at all.\n",
    "\n",
    "Target is to train a model to differentiate links and bases from non-fitting drawings. For this task a dataset consisting of about 500 examples each was created.\n",
    "I want to go through *n* TODO different steps to show, that the model can differentiate *non-links* from *links*. Then the same algortithm is used to determine *bases*.\n",
    "\n",
    "This is the first try in a series of steps taken to create a neural network to identify fourbar linkages from sketches and map them to their digital counterparts.\n",
    "\n",
    "The steps taken are as follows:\n",
    " 1. Acquire data from local harddrive (and then show loaded images).\n",
    " 2. Prepare data by creating tensors of image, label pairs.\n",
    " 3. Create a simple CNN to classify \"links\" from \"non-hits\" (*o*'s from *n*'s).\n",
    " 4. Train model.\n",
    " 5. Evaluate results.\n",
    " \n",
    "After these four steps the model should be trained with a variety of hyperparameters to see which is the most promising one.\n",
    "\n",
    "-> Further steps will try to use these models inside another CNN to get the coordinate of hits in a sketch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "Acquire Data and put them in proper directories to train on them.\n",
    "\n",
    "Data is stored in ../data/{n, o, x} with either \"no match\", \"joints\" or \"bases\" respectively.\n",
    "It is not in the working directory, because multiple approaches (with different programming languages) are sought to be used on this dataset.\n",
    "\n",
    "At first the right environment is created inside the working directory\n",
    "\n",
    "- data\n",
    "    - train\n",
    "        - n\n",
    "        - o\n",
    "        - x\n",
    "    - validate\n",
    "        - n\n",
    "        - o\n",
    "        - x\n",
    "    - test\n",
    "        - n\n",
    "        - o\n",
    "        - x\n",
    "        \n",
    "In these folders a subset of the *linkages* or *bases* and *non-hits* are placed to be used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, exists\n",
    "from os import mkdir\n",
    "\n",
    "def mkdir_ex(path):\n",
    "    if not exists(path):\n",
    "        mkdir(path)\n",
    "\n",
    "n_dir = join('..', 'data', 'n') # non hits\n",
    "o_dir = join('..', 'data', 'o') # links\n",
    "# x_dir = join('..', 'data', 'x') # bases\n",
    "\n",
    "data = join('.', 'data')\n",
    "mkdir_ex(data)\n",
    "# Create bases directories for training, validation and testing\n",
    "train_dir = join(data, 'train')\n",
    "mkdir_ex(train_dir)\n",
    "validation_dir = join(data, 'validation')\n",
    "mkdir_ex(validation_dir)\n",
    "test_dir = join(data, 'test')\n",
    "mkdir_ex(test_dir)\n",
    "# Create respective training directories for data\n",
    "train_nohit = join(train_dir, 'n')\n",
    "mkdir_ex(train_nohit)\n",
    "train_links = join(train_dir, 'o')\n",
    "mkdir_ex(train_links)\n",
    "# train_bases = join(train_dir, 'x')\n",
    "# mkdir_ex(train_bases)\n",
    "# And validation directories\n",
    "validate_nohit = join(validation_dir, 'n')\n",
    "mkdir_ex(validate_nohit)\n",
    "validate_links = join(validation_dir, 'o')\n",
    "mkdir_ex(validate_links)\n",
    "# validate_bases = join(validation_dir, 'x')\n",
    "# mkdir_ex(validate_bases)\n",
    "# And test directories\n",
    "test_nohit = join(test_dir, 'n')\n",
    "mkdir_ex(test_nohit)\n",
    "test_links = join(test_dir, 'o')\n",
    "mkdir_ex(test_links)\n",
    "# test_bases = join(test_dir, 'x')\n",
    "# mkdir_ex(test_bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all folders are created and ready to be filled, the data is now propagated to their directories.\n",
    "\n",
    "The dataset consists of at least 500 entries each.\n",
    "To be exact, we take 500 images and distribute them about 60/20/20 into training, validation and test. This means each set brings:\n",
    "300 entries into training.\n",
    "100 entries into validation.\n",
    "100 entries into test.\n",
    "\n",
    "Another helpful aspect is, that the original data stays untouched and can not be compromised in any way.\n",
    "\n",
    "To increase the number of data via augmentation is a subject of later debate, if there is improvement to be expected.\n",
    "\n",
    "Since all data is named {0,1,2,3,4,5...}.jpeg inside their labelset, we can use this property to easily distribute the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def distribute_data(target_dir, src_dir, begin, limit):\n",
    "    for i in range(begin, limit):\n",
    "        filename = str(i) + '.jpeg'\n",
    "        src = join(src_dir, filename)\n",
    "        target = join(target_dir, filename)\n",
    "        copyfile(src, target)\n",
    "\n",
    "distribute_data(train_nohit, n_dir, 0, 300)\n",
    "distribute_data(train_links, o_dir, 0, 300)\n",
    "# distribute_data(train_bases, x_dir, 0, 300)\n",
    "distribute_data(validate_nohit, n_dir, 300, 400)\n",
    "distribute_data(validate_links, o_dir, 300, 400)\n",
    "# distribute_data(validate_bases, x_dir, 300, 400)\n",
    "distribute_data(test_nohit, n_dir, 400, 500)\n",
    "distribute_data(test_links, o_dir, 400, 500)\n",
    "# distribute_data(test_links, x_dir, 400, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Preprocess data to be fit to be used. (Maybe data preprocessing is better to be done after model definition, because the model determines the input shape).\n",
    "\n",
    "The data has to be transformed into tensors which can be fed into the model.\n",
    "Four steps are suggested by the book (p.135):\n",
    " - Read the picture files.\n",
    " - Decode the JPEG content to RGB grids of pixels.\n",
    " - Convert these into floating-point tensors.\n",
    " - Rescale the pixel values (between 0 and 255) to the [0, 1] interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 images belonging to 2 classes.\n",
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(512, 512),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(512, 512),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Now a generic model for testing is created.\n",
    "\n",
    "Here the model from deep learning with python p. 134 is used.\n",
    "\n",
    "This should be reduced later an analyzed on my own. But for a quick proof of concept this should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0910 23:25:54.580121 139773052057216 deprecation.py:506] From /usr/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 510, 510, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 255, 255, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 253, 253, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 126, 126, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 124, 124, 128)     73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 62, 62, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 60, 60, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 115200)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               58982912  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 59,224,257\n",
      "Trainable params: 59,224,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(512, 512, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model should be configured for training.\n",
    "Therefore optimizers are imported. For binary classification the loss function 'binary_crossentropy' and as optimizer 'RMSprop' is used.\n",
    "\n",
    "\n",
    "This is recommended by Francois Chollet. Why this is the case is a matter of further research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0910 23:25:54.748359 139773052057216 deprecation.py:323] From /usr/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=1e-4), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "Training the model is done via the \"fit\" method. for this the train_generator has to be used.\n",
    "\n",
    "## Disclaimer: At the moment train_generator has 3 classes. I have to select 2 of them!\n",
    "\n",
    "Tensorboard should be used for visualisation. Therefore a log directory is created, with a suitable callback object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 34s 336ms/step - loss: 0.3114 - acc: 0.8920 - val_loss: 0.2924 - val_acc: 0.9050\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 30s 298ms/step - loss: 0.1190 - acc: 0.9585 - val_loss: 0.2976 - val_acc: 0.9100\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 30s 301ms/step - loss: 0.0717 - acc: 0.9730 - val_loss: 0.3714 - val_acc: 0.9050\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 30s 299ms/step - loss: 0.0413 - acc: 0.9840 - val_loss: 0.4515 - val_acc: 0.8950\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 30s 298ms/step - loss: 0.0372 - acc: 0.9855 - val_loss: 0.8512 - val_acc: 0.9100\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 30s 301ms/step - loss: 0.0174 - acc: 0.9950 - val_loss: 1.1141 - val_acc: 0.9100\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 0.0199 - acc: 0.9925 - val_loss: 1.8900 - val_acc: 0.8950\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 30s 301ms/step - loss: 0.0146 - acc: 0.9960 - val_loss: 1.8830 - val_acc: 0.8950\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 0.0233 - acc: 0.9930 - val_loss: 1.8309 - val_acc: 0.9050\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 0.0101 - acc: 0.9965 - val_loss: 1.9046 - val_acc: 0.9050\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 30s 303ms/step - loss: 0.0092 - acc: 0.9975 - val_loss: 2.1141 - val_acc: 0.9100\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 30s 303ms/step - loss: 0.0063 - acc: 0.9985 - val_loss: 2.2901 - val_acc: 0.9100\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 30s 303ms/step - loss: 0.0087 - acc: 0.9980 - val_loss: 2.5563 - val_acc: 0.9150\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 0.0040 - acc: 0.9985 - val_loss: 3.1153 - val_acc: 0.9000\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 31s 306ms/step - loss: 6.4841e-04 - acc: 0.9995 - val_loss: 7.9026 - val_acc: 0.7600\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 0.0239 - acc: 0.9970 - val_loss: 2.8603 - val_acc: 0.9050\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 31s 305ms/step - loss: 3.7720e-04 - acc: 1.0000 - val_loss: 3.2568 - val_acc: 0.9050\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 31s 306ms/step - loss: 1.3991e-04 - acc: 1.0000 - val_loss: 3.8508 - val_acc: 0.9050\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 31s 306ms/step - loss: 0.0069 - acc: 0.9995 - val_loss: 4.3735 - val_acc: 0.8950\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 0.0049 - acc: 0.9985 - val_loss: 3.8914 - val_acc: 0.9100\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 30s 305ms/step - loss: 2.0527e-05 - acc: 1.0000 - val_loss: 4.3400 - val_acc: 0.9100\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 31s 306ms/step - loss: 0.0012 - acc: 0.9995 - val_loss: 5.2188 - val_acc: 0.9000\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 31s 307ms/step - loss: 1.1348e-06 - acc: 1.0000 - val_loss: 5.9582 - val_acc: 0.8650\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 31s 307ms/step - loss: 0.0016 - acc: 0.9990 - val_loss: 6.2229 - val_acc: 0.8950\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 31s 307ms/step - loss: 6.3888e-07 - acc: 1.0000 - val_loss: 5.9968 - val_acc: 0.8900\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 31s 306ms/step - loss: 0.0061 - acc: 0.9980 - val_loss: 6.8584 - val_acc: 0.8800\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 31s 306ms/step - loss: 2.8324e-06 - acc: 1.0000 - val_loss: 6.3629 - val_acc: 0.8950\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 0.0233 - acc: 0.9985 - val_loss: 7.5131 - val_acc: 0.8550\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 30s 305ms/step - loss: 0.0060 - acc: 0.9990 - val_loss: 4.5652 - val_acc: 0.9100\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 30s 305ms/step - loss: 1.8285e-06 - acc: 1.0000 - val_loss: 4.8730 - val_acc: 0.9050\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "\n",
    "log_dir=(join('.', 'logs'))\n",
    "mkdir_ex(log_dir)\n",
    "\n",
    "callbacks = [ TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    embeddings_freq=1) ]\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50 )#,\n",
    "    # callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
