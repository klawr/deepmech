% Encoding: UTF-8

@WWW{Goessner2019,
  author = {Stefan Gössner},
  title  = {mec2},
  year   = {2019},
  url    = {https://github.com/goessner/mec2},
}

@Book{Goessner2019a,
  author    = {Stefan Gössner},
  title     = {Ebene Mechanismenmodelle als Partikelsysteme - ein neuer Ansatz. Tagungsband 13. Kolloquium Getriebetechnik, Fachhochschule Dortmund, 18. - 20. September 2019},
  year      = {2019},
  publisher = {Logos Berlin},
  isbn      = {978-3-8325-4979-4},
  pages     = {169-180},
  url       = {https://www.amazon.com/Tagungsband-Kolloquium-Getriebetechnik-Fachhochschule-Dortmund/dp/383254979X?SubscriptionId=AKIAIOBINVZYXZQZ2U3A&tag=chimbori05-20&linkCode=xm2&camp=2025&creative=165953&creativeASIN=383254979X},
}

@Book{Goessner2018,
  author = {Stefan Gössner},
  title  = {Fundamentals for Web-Based Analysis and Simulation of Planar Mechanisms, EuCoMeS 2018, Proceedings of the 7th European Conference of Mechanism Science},
  year   = {2018},
}

@WWW{Goessner2019b,
  author = {Stefan Gössner},
  title  = {g2},
  year   = {2019},
  url    = {https://github.com/goessner/g2},
}

@MastersThesis{Uhlig2019,
  author = {Jan Uhlig},
  title  = {Entwicklung einer modularen Web-App zur interaktiven Modellierung und impulsbasierten Analyse beliebiger planarer Koppelmechanismen},
  year   = {2019},
  school = {Fachhochschule Dortmund},
}

@WWW{Uhlig2019a,
  author  = {Jan Uhlig},
  title   = {mecEdit},
  year    = {2019},
  url     = {https://mecedit.com},
  urldate = {2019-12-11},
}

@WWW{Google2019,
  author  = {Google},
  title   = {Tensorflow},
  date    = {2019},
  url     = {https://www.tensorflow.org/},
  urldate = {2019-10-09},
}

@WWW{Chollet2019,
  author  = {François Chollet},
  title   = {Keras},
  year    = {2019},
  url     = {https://keras.io/},
  urldate = {2019-10-09},
}

@Book{Chollet2017,
  author    = {Chollet, Francois},
  title     = {Deep Learning with Python},
  year      = {2017},
  date      = {2017-10-28},
  publisher = {Manning Publications},
  isbn      = {1617294438},
  pagetotal = {384},
  url       = {https://www.ebook.de/de/product/28930398/francois_chollet_deep_learning_with_python.html},
  ean       = {9781617294433},
}

@Article{Long2014,
  author      = {Jonathan Long and Evan Shelhamer and Trevor Darrell},
  title       = {Fully Convolutional Networks for Semantic Segmentation},
  date        = {2014-11-14},
  eprint      = {http://arxiv.org/abs/1411.4038v2},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1411.4038v2:PDF},
  keywords    = {cs.CV},
}

@WWW{nvidia2019,
  author  = {nvidia},
  title   = {CUDA},
  year    = {2019},
  url     = {https://www.geforce.com/hardware/technology/cuda},
  urldate = {2019-10-09},
}

@Book{StuartRussell2018,
  author    = {Stuart Russell, Peter Norvig},
  title     = {Artificial Intelligence: A Modern Approach, Global Edition},
  year      = {2018},
  date      = {2018-11-28},
  publisher = {Addison Wesley},
  isbn      = {1292153962},
  url       = {https://www.ebook.de/de/product/25939961/stuart_russell_peter_norvig_artificial_intelligence_a_modern_approach_global_edition.html},
  ean       = {9781292153964},
}

@Article{Redmon2018,
  author      = {Joseph Redmon and Ali Farhadi},
  title       = {YOLOv3: An Incremental Improvement},
  date        = {2018-04-08},
  eprint      = {http://arxiv.org/abs/1804.02767v1},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
  file        = {:http\://arxiv.org/pdf/1804.02767v1:PDF},
  keywords    = {cs.CV},
}

@Article{Redmon2015,
  author      = {Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
  title       = {You Only Look Once: Unified, Real-Time Object Detection},
  date        = {2015-06-08},
  eprint      = {http://arxiv.org/abs/1506.02640v5},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  file        = {:http\://arxiv.org/pdf/1506.02640v5:PDF},
  keywords    = {cs.CV},
}

@Article{Redmon2016,
  author      = {Joseph Redmon and Ali Farhadi},
  title       = {YOLO9000: Better, Faster, Stronger},
  date        = {2016-12-25},
  eprint      = {http://arxiv.org/abs/1612.08242v1},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
  file        = {:http\://arxiv.org/pdf/1612.08242v1:PDF},
  keywords    = {cs.CV},
}

@Comment{jabref-meta: databaseType:biblatex;}
