% Encoding: UTF-8

@Book{StuartRussell2018,
  author    = {Stuart Russell, Peter Norvig},
  title     = {Artificial Intelligence: A Modern Approach, Global Edition},
  year      = {2018},
  date      = {2018-11-28},
  publisher = {Addison Wesley},
  isbn      = {1292153962},
  url       = {https://www.ebook.de/de/product/25939961/stuart_russell_peter_norvig_artificial_intelligence_a_modern_approach_global_edition.html},
  ean       = {9781292153964},
}

@Book{Chollet2017,
  author    = {Chollet, Francois},
  title     = {Deep Learning with Python},
  year      = {2017},
  date      = {2017-10-28},
  publisher = {Manning Publications},
  isbn      = {1617294438},
  pagetotal = {384},
  url       = {https://www.ebook.de/de/product/28930398/francois_chollet_deep_learning_with_python.html},
  ean       = {9781617294433},
}

@Book{Goodfellow2017,
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  title     = {Deep Learning},
  year      = {2017},
  date      = {2017-01-03},
  publisher = {The MIT Press},
  isbn      = {0262035618},
  pagetotal = {800},
  url       = {https://www.ebook.de/de/product/26337726/ian_goodfellow_yoshua_bengio_aaron_courville_deep_learning.html},
  ean       = {9780262035613},
}

@Article{McCulloch1943,
  author   = {McCulloch, Warren S. and Pitts, Walter},
  title    = {A logical calculus of the ideas immanent in nervous activity},
  journal  = {The bulletin of mathematical biophysics},
  year     = {1943},
  volume   = {5},
  number   = {4},
  month    = {Dec},
  pages    = {115--133},
  issn     = {1522-9602},
  doi      = {10.1007/BF02478259},
  url      = {https://doi.org/10.1007/BF02478259},
  abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  day      = {01},
}

@WWW{McCarthy1955,
  author  = {John McCarthy},
  title   = {A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence},
  year    = {1955},
  url     = {http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html},
  urldate = {2019-10-09},
}

@WWW{Chollet,
  author  = {François Chollet},
  title   = {Keras},
  year    = {2019},
  url     = {https://keras.io/},
  urldate = {2019-10-09},
}

@WWW{Google2019,
  author  = {Google},
  title   = {Tensorflow},
  date    = {2019},
  url     = {https://www.tensorflow.org/},
  urldate = {2019-10-09},
}

@WWW{nvidia2019,
  author  = {nvidia},
  title   = {CUDA},
  year    = {2019},
  url     = {https://www.geforce.com/hardware/technology/cuda},
  urldate = {2019-10-09},
}

@Article{Cybenko1989,
  author   = {Cybenko, G.},
  title    = {Approximation by superpositions of a sigmoidal function},
  journal  = {Mathematics of Control, Signals and Systems},
  year     = {1989},
  volume   = {2},
  number   = {4},
  month    = {Dec},
  pages    = {303--314},
  issn     = {1435-568X},
  doi      = {10.1007/BF02551274},
  url      = {https://doi.org/10.1007/BF02551274},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  day      = {01},
}

@Article{Hornik1989,
  author    = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
  title     = {Multilayer feedforward networks are universal approximators},
  journal   = {Neural Networks},
  year      = {1989},
  volume    = {2},
  number    = {5},
  month     = {jan},
  pages     = {359--366},
  doi       = {10.1016/0893-6080(89)90020-8},
  publisher = {Elsevier {BV}},
}

@WWW{Good1965,
  author  = {Irvin John Good},
  title   = {Speculations Concerning the First Ultraintelligent Machine},
  year    = {1965},
  url     = {http://acikistihbarat.com/dosyalar/artificial-intelligence-first-paper-on-intelligence-explosion-by-good-1964-acikistihbarat.pdf},
  urldate = {2019-10-16},
}

@Article{Rolnick2017,
  author      = {David Rolnick and Max Tegmark},
  title       = {The power of deeper networks for expressing natural functions},
  journal     = {Neural Networks},
  year        = {2017},
  date        = {2017-05-16},
  eprint      = {http://arxiv.org/abs/1705.05502v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {It is well-known that neural networks are universal approximators, but that deeper networks tend in practice to be more powerful than shallower ones. We shed light on this by proving that the total number of neurons $m$ required to approximate natural classes of multivariate polynomials of $n$ variables grows only linearly with $n$ for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from $1$ to $k$, the neuron requirement grows exponentially not with $n$ but with $n^{1/k}$, suggesting that the minimum number of layers required for practical expressibility grows only logarithmically with $n$.},
  file        = {:http\://arxiv.org/pdf/1705.05502v2:PDF},
  keywords    = {cs.LG, cs.NE, stat.ML},
}

@Article{Duchi2010,
  author  = {John Duchi and Elad Hazan and Yoram Singer},
  title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal = {Journal of Machine Learning Research 12},
  year    = {2010},
  url     = {http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf},
}

@Article{Rumelhart1986,
  author  = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  title   = {Learning representations by back-propagating errors},
  journal = {Nature, Volume 323, Issue 6088, pp. 533-536 (1986)},
  year    = {1986},
  volume  = {323},
  month   = oct,
  pages   = {533-536},
  doi     = {10.1038/323533a0},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl  = {https://ui.adsabs.harvard.edu/abs/1986Natur.323..533R},
}

@Article{Glorot2011,
  author  = {Glorot, Xavier and Bordes, Antoine and Bengio, Y.},
  title   = {Deep Sparse Rectifier Neural Networks},
  journal = {Proceedings of the 14th International Conference on Artificial Intelligence and Statisitics (AISTATS) 2011},
  year    = {2011},
  volume  = {15},
  month   = {01},
  pages   = {315-323},
}

@WWW{Nielsen2015,
  author  = {Michael A. Nielsen},
  title   = {Neural Networks and Deep Learning},
  year    = {2015},
  url     = {http://neuralnetworksanddeeplearning.com},
  urldate = {2019-10-17},
}

@Article{Halevy2009,
  author   = {A. {Halevy} and P. {Norvig} and F. {Pereira}},
  title    = {The Unreasonable Effectiveness of Data},
  journal  = {IEEE Intelligent Systems},
  year     = {2009},
  volume   = {24},
  number   = {2},
  month    = {March},
  pages    = {8-12},
  doi      = {10.1109/MIS.2009.36},
  keywords = {data handling;Internet;natural language processing;data unreasonable effectiveness;Brown Corpus;English words;trillion-word corpus;frequency counts;unfiltered Web pages;incomplete sentences;spelling errors;grammatical errors;hand-corrected part-of-speech tags;Web-derived corpora;Humans;Data mining;Machine learning;Speech recognition;Frequency estimation;Web pages;Videos;Broadcasting;Natural language processing;Tagging;machine learning;very large data bases;Semantic Web},
}

@InProceedings{Banko2001,
  author    = {Banko, Michele and Brill, Eric},
  title     = {Scaling to Very Very Large Corpora for Natural Language Disambiguation},
  booktitle = {Proceedings of the 39th Annual Meeting on Association for Computational Linguistics},
  year      = {2001},
  series    = {ACL '01},
  publisher = {Association for Computational Linguistics},
  location  = {Toulouse, France},
  pages     = {26--33},
  doi       = {10.3115/1073012.1073017},
  url       = {https://doi.org/10.3115/1073012.1073017},
  acmid     = {1073017},
  address   = {Stroudsburg, PA, USA},
  numpages  = {8},
}

@WWW{OpenCV2019,
  author  = {OpenCV},
  title   = {OpenCV},
  year    = {2019},
  url     = {https://opencv.org/},
  urldate = {2019-10-22},
}

@WWW{Heinisuo2019,
  author  = {Olli-Pekka Heinisuo},
  title   = {opencv-python},
  year    = {2019},
  url     = {https://github.com/skvark/opencv-python},
  urldate = {2019-10-22},
}

@WWW{drivendata2019,
  author  = {drivendata},
  title   = {Cookiecutter Data Science},
  date    = {2019},
  url     = {https://drivendata.github.io/cookiecutter-data-science/},
  urldate = {2019-10-23},
}

@WWW{Jupyter2019,
  author  = {Jupyter},
  title   = {Jupyter Notebook},
  year    = {2019},
  url     = {https://jupyter.org/},
  urldate = {2019-10-23},
}

@InProceedings{Sutskever2013,
  author    = {Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton},
  title     = {On the importance of initialization and momentum in deep learning},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  year      = {2013},
  editor    = {Sanjoy Dasgupta and David McAllester},
  volume    = {28},
  series    = {Proceedings of Machine Learning Research},
  number    = {3},
  publisher = {PMLR},
  pages     = {1139--1147},
  url       = {http://proceedings.mlr.press/v28/sutskever13.html},
  abstract  = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.   },
  address   = {Atlanta, Georgia, USA},
  file      = {sutskever13.pdf:http\://proceedings.mlr.press/v28/sutskever13.pdf:PDF},
}

@Article{Nesterov1983,
  author  = {Nesterov, Y. E.},
  title   = {A method for solving the convex programming problem with convergence rate O(1/k^2)},
  journal = {Dokl. Akad. Nauk SSSR},
  year    = {1983},
  volume  = {269},
  pages   = {543-547},
  url     = {https://ci.nii.ac.jp/naid/10029946121/en/},
}

@Book{Geron2019,
  author    = {Géron, Aurélien},
  title     = {Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow},
  year      = {2019},
  date      = {2019-10-01},
  publisher = {O'Reilly UK Ltd.},
  isbn      = {1492032646},
  pagetotal = {819},
  url       = {https://www.ebook.de/de/product/33315532/aurelien_geron_hands_on_machine_learning_with_scikit_learn_keras_and_tensorflow.html},
  ean       = {9781492032649},
}

@Article{Polyak1964,
  author  = {Polyak, Boris},
  title   = {Some methods of speeding up the convergence of iteration methods},
  journal = {Ussr Computational Mathematics and Mathematical Physics},
  year    = {1964},
  volume  = {4},
  month   = {12},
  pages   = {1-17},
  doi     = {10.1016/0041-5553(64)90137-5},
}

@Book{Bellman1957,
  author    = {Bellman, Richard E.},
  title     = {Dynamic Programming},
  year      = {1957},
  date      = {1957-10-11},
  publisher = {PRINCETON UNIV PR},
  isbn      = {069107951X},
  pagetotal = {342},
  url       = {https://www.ebook.de/de/product/34448612/richard_e_bellman_dynamic_programming.html},
  ean       = {9780691079516},
}

@Article{Wolpert1996,
  author   = {Wolpert, David H.},
  title    = {The Lack of A Priori Distinctions Between Learning Algorithms},
  journal  = {Neural Computation},
  year     = {1996},
  volume   = {8},
  number   = {7},
  pages    = {1341-1390},
  doi      = {10.1162/neco.1996.8.7.1341},
  eprint   = {https://doi.org/10.1162/neco.1996.8.7.1341},
  url      = { 
        https://doi.org/10.1162/neco.1996.8.7.1341
    
},
  abstract = { This is the first of two papers that use off-training set (OTS) error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no a priori distinctions between learning algorithms. (The second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are “as many” targets (or priors over targets) for which A has lower expected OTS error than B as vice versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is “anti-cross-validation” (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one cannot say: if empirical misclassification rate is low, the Vapnik-Chervonenkis dimension of your generalizer is small, and the training set is large, then with high probability your OTS error is small. Other implications for “membership queries” algorithms and “punting” algorithms are also discussed. },
}

@Article{Lecun1998,
  author    = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
  title     = {Gradient-based learning applied to document recognition},
  journal   = {Proceedings of the {IEEE}},
  year      = {1998},
  volume    = {86},
  number    = {11},
  pages     = {2278--2324},
  doi       = {10.1109/5.726791},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@WWW{Google2019a,
  author  = {Google},
  title   = {Keras Tuner},
  year    = {2019},
  url     = {https://keras-team.github.io/keras-tuner/},
  urldate = {2019-11-28},
}

@Article{Li2018,
  author  = {Lisha Li and Kevin Jamieson and Giulia DeSalvo and Afshin Rostamizadeh and Ameet Talwalkar},
  title   = {Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {185},
  pages   = {1-52},
  url     = {http://jmlr.org/papers/v18/16-558.html},
}

@Unpublished{Hinton2012,
  author = {Geoffrey Hinton , Nitish Srivastava, Kevin Swersky},
  title  = {Neural Networks for Machine Learning},
  year   = {2012},
  url    = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
}

@Misc{Kingma2014,
  author        = {Diederik P. Kingma and Jimmy Ba},
  title         = {Adam: A Method for Stochastic Optimization},
  year          = {2014},
  eprint        = {1412.6980},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
}

@InProceedings{Reddi2018,
  author    = {Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
  title     = {On the Convergence of Adam and Beyond},
  booktitle = {International Conference on Learning Representations},
  year      = {2018},
  url       = {https://openreview.net/forum?id=ryQu7f-RZ},
}

@MastersThesis{Uhlig2019,
  author = {Jan Uhlig},
  title  = {Entwicklung einer modularen Web-App zur interaktiven Modellierung und impulsbasierten Analyse beliebiger planarer Koppelmechanismen},
  year   = {2019},
  school = {Fachhochschule Dortmund},
}

@WWW{Goessner2019,
  author = {Stefan Gössner},
  title  = {mec2},
  year   = {2019},
  url    = {https://github.com/goessner/mec2},
}

@Article{Zeiler2012,
  author        = {Matthew D. Zeiler},
  title         = {{ADADELTA:} An Adaptive Learning Rate Method},
  journal       = {CoRR},
  year          = {2012},
  volume        = {abs/1212.5701},
  eprint        = {1212.5701},
  url           = {http://arxiv.org/abs/1212.5701},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1212-5701},
  timestamp     = {Mon, 13 Aug 2018 16:45:57 +0200},
}

@InProceedings{Goessner2019a,
  author    = {Gössner, Stefan},
  title     = {Fundamentals for Web-Based Analysis and Simulation of Planar Mechanisms},
  booktitle = {EuCoMeS 2018},
  year      = {2019},
  editor    = {Corves, Burkhard and Wenger, Philippe and H{\"u}sing, Mathias},
  publisher = {Springer International Publishing},
  isbn      = {978-3-319-98020-1},
  pages     = {215--222},
  abstract  = {With the growing importance of distance education comes an increasing demand for web-based tools for analysis and simulation of mechanisms.},
  address   = {Cham},
}

@InProceedings{Goessner2019b,
  author    = {Gössner, Stefan},
  title     = {Ebene Mechanismenmodelle als Partikelsysteme - ein neuer Ansatz},
  booktitle = {13. Kolloquium-Getriebetechnik - Tagungsband},
  year      = {2019},
  editor    = {Corves, Burkhard and Wenger, Philippe and H{\"u}sing, Mathias},
  publisher = {Springer International Publishing},
  isbn      = {978-3-8325-4979-4},
  pages     = {169-180},
  abstract  = {With the growing importance of distance education comes an increasing demand for web-based tools for analysis and simulation of mechanisms.},
  address   = {Cham},
}

@WWW{Uhlig2019a,
  author  = {Jan Uhlig},
  title   = {mecEdit},
  year    = {2019},
  url     = {https://mecedit.com},
  urldate = {2019-12-11},
}

@Comment{jabref-meta: databaseType:biblatex;}
