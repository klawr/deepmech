{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## manual data augmentation\n",
    "\n",
    "In this notebook the code written to augmentate data is consolidated into a few lines of code.\n",
    "The respective scripts can be reviewed at `src/image_handling`.\n",
    "`src.utils` should not need any further explanation.\n",
    "\n",
    "The code is basically emulation the behavior of the keras image generator, but also accepts another parameter `repetitions`.\n",
    "The data is processed into the interim folder applying the modifications before training.\n",
    "Therefore the manipulations do not take place during training, but a time independent preprocessing phase.\n",
    "\n",
    "The images are copied to the `interim` directory first.\n",
    "After the data is fully in `interim`, the data gets augmented \"inplace\", meaning we only work with the `interim` data and the result is a `interim` directory with all the augmented data (in this case 32000 images per class).\n",
    "\n",
    "`inline_augment_images` returns a list with dictionaries.\n",
    "These dictionaries contain all necessary information to create records in the upcoming cell.\n",
    "\n",
    "The parameters should be self explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from os.path import join\n",
    "\n",
    "from src.image_handling import inline_augment_images, encode_record\n",
    "from src.utils import reset_and_distribute_data\n",
    "\n",
    "raw=join('data', 'raw')\n",
    "interim=join('data', 'interim')\n",
    "processed=join('data', 'processed')\n",
    "\n",
    "reset_and_distribute_data(raw, interim, [1000, 100, 100])\n",
    "shutil.rmtree(processed, ignore_errors=True)\n",
    "\n",
    "target_size=(32, 32)\n",
    "\n",
    "train = inline_augment_images(join(interim, 'train'),\n",
    "    repetitions=32, h_flip=True, v_flip=True, rotation_range=360, target_size=target_size)\n",
    "\n",
    "validation = inline_augment_images(join(interim, 'validation'), target_size=target_size)\n",
    "\n",
    "test = inline_augment_images(join(interim, 'test'), target_size=target_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`encode_record` takes the previously described list, which is hardcoded in this instance as:\n",
    "```python\n",
    "feature = {\n",
    "    'image': # /path/to/image\n",
    "    'label': # 1 or 0\n",
    "    'angle': # a number: [0, 180)\n",
    "}\n",
    "```\n",
    "\n",
    "These features are then used to create protobufs.\n",
    "protobufs can be read by Tensorflow very efficient and with no overhead of decoding the data (the decoding takes place during the preprocessing here).\n",
    "\n",
    "This procedure also gives way more control over how the data is stored. For example the angle in this instance is saved, because maybe in the future it may be interesting to determine the angle of a linear node, where the data is way easier to generate by using only vertical lines (and the randomization is done by the augmentation).\n",
    "\n",
    "The next step is to create data generators for the model which can read these tensorflow records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "labels = listdir(raw)\n",
    "\n",
    "encode_record(train, labels, processed, 'train')\n",
    "encode_record(validation, labels, processed, 'validation')\n",
    "encode_record(test, labels, processed, 'test')"
   ]
  }
 ]
}