{"cells":[{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## first model\n","\n","This notebook is regarding the homonymous fifth chapter of the accompanying report, located at `reports/srp/document.pdf`.\n","\n","The first model can be considered as proof of concept.\n","It is a simple model composed of only dense layers, manually tuned to represent a somewhat working model."]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["The first cell loads the data from `data/raw` to `data/processed`.\n","The `intermediate` directory is not regarded here, since no manipulations are done on the images themself.\n","\n","For this some helping function to create the training environment are defined, which can be found in `src/training_env.py`.\n","\n","`reset_and_populate` resets the `processed` folder and then distributes data from `raw`. In this case 500 images are taken from `raw`, whereby 400 images from each class (`n`, `o` and `x`) are put into training, no images are put into validation (because no validation takes place in this notebook) and 100 images of each class are taken for testing.\n","\n","Please note that `raw` contains the classes as directories as it is suggested by the `ImageDataGenerator` for the processed data; e.g. all images for the bases are in `data/raw/x/`."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":"['data\\\\processed\\\\train\\\\n',\n 'data\\\\processed\\\\validation\\\\n',\n 'data\\\\processed\\\\test\\\\n',\n 'data\\\\processed\\\\train\\\\o',\n 'data\\\\processed\\\\validation\\\\o',\n 'data\\\\processed\\\\test\\\\o',\n 'data\\\\processed\\\\train\\\\x',\n 'data\\\\processed\\\\validation\\\\x',\n 'data\\\\processed\\\\test\\\\x']"},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from os.path import join\n","\n","raw = join('data', 'raw')\n","processed = join('data', 'processed')\n","\n","from src.training_env import reset_and_populate\n","\n","reset_and_populate(raw, processed, [400,0,100])"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["After loading a `generator` is defined, which can be passed to the models `fit` function later in the code.\n","\n","At this point two generators are created. One for the training data, which is used for training and possibly submitted multiple times and one for testing which is never looked at besides for model evaluation.\n","\n","The `create_generator` function takes the image size and the batch size.\n","The image size is reduced at this point to shrink the information given into the model to a small, but still reasonable size. The size of the batch is determining how many images are given to the model to calculate the loss before the weights and biases are updated.\n","\n","It can be seen that the classes are detected as described earlier, as it found 1200 images (400 x 3) classes in `train` and 300 images (100 x 3) in `test`."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Found 1200 images belonging to 3 classes.\nFound 300 images belonging to 3 classes.\n"}],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","def create_generator(data_dir, batch_size):\n","    datagen = ImageDataGenerator(rescale=1./255)\n","    full_path = join(processed, data_dir)\n","    return datagen.flow_from_directory(\n","        full_path,\n","        target_size=(32, 32),\n","        batch_size=batch_size,\n","        class_mode='binary')\n","\n","train_generator = create_generator('train', 20)\n","test_generator = create_generator('test', 10)"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["After the data and the respective generators are prepared, the model is created.\n","\n","For the beginning the model is a simple feed forward neural network with only dense layers.\n","The input shape is predetermined by the data given to the model.\n","If the data does not fit to the input layer, the training (and the evaluation) will not work.\n","\n","The model has 3 dense layers, whereas the activation is given by `relu`.\n","These values are more or less lucky guesses at this point to create a somewhat feasible model to show that this process actually works.\n","\n","The `softmax` activation in the last layer takes the values of the last layer and normalizes them.\n","This is necessary to compare the values with the labels (which are given as 1 for the correct label and 0 for the others)."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten (Flatten)            (None, 3072)              0         \n_________________________________________________________________\ndense (Dense)                (None, 32)                98336     \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                1056      \n_________________________________________________________________\ndense_2 (Dense)              (None, 3)                 99        \n=================================================================\nTotal params: 99,491\nTrainable params: 99,491\nNon-trainable params: 0\n_________________________________________________________________\n"}],"source":["from tensorflow.keras import layers\n","from tensorflow.keras import models\n","\n","model = models.Sequential()\n","model.add(layers.Flatten(input_shape=(32, 32, 3)))\n","model.add(layers.Dense(32,'relu'))\n","model.add(layers.Dense(32,'relu'))\n","model.add(layers.Dense(3, 'softmax'))\n","\n","model.summary()"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["The optimizer used in this example is gradient descent, which was already introduced in chapter 2. It is called \"stochastic\" despite the batch size being greater than one.\n","\n","The learning rate and momentum are tweaked manually, which is not optimal but suffices for a first test.\n","In first manual tests it seems that to set nesterov to `True` results in better results.\n","\n","In later chapters and notebooks a more sophistiated look is taken on these hyper-parameters.\n","\n","The loss function is `sparse_categorical_crossentropy`, which calculates sum of the differences between the prediction and the label for each class."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from tensorflow.keras.optimizers import SGD, RMSprop\n","\n","optimizer = SGD(lr=0.005, momentum=0.9, nesterov=True)\n","\n","model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['acc'])"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["In this cell the log directory is defined.\n","The log directory is a concatenation of the project abbreviation (srp) and the chapter this notebook is written for.\n","\n","Furthermore a directory is created for the current time to be able to distinguish different runs.\n","\n","A callback for Tensorboard is implemented to further analyze the training process.\n","\n","`model.fit` initializes the training."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Epoch 1/20\n20/20 [==============================] - 2s 79ms/step - loss: 1.1220 - acc: 0.3425\nEpoch 2/20\n20/20 [==============================] - 1s 55ms/step - loss: 1.0457 - acc: 0.4675\nEpoch 3/20\n20/20 [==============================] - 1s 55ms/step - loss: 0.9199 - acc: 0.5800\nEpoch 4/20\n20/20 [==============================] - 1s 46ms/step - loss: 0.8548 - acc: 0.6950\nEpoch 5/20\n20/20 [==============================] - 1s 50ms/step - loss: 0.6464 - acc: 0.8100\nEpoch 6/20\n20/20 [==============================] - 1s 49ms/step - loss: 0.6402 - acc: 0.7975\nEpoch 7/20\n20/20 [==============================] - 1s 49ms/step - loss: 0.5478 - acc: 0.8325\nEpoch 8/20\n20/20 [==============================] - 1s 46ms/step - loss: 0.5328 - acc: 0.8100\nEpoch 9/20\n20/20 [==============================] - 1s 51ms/step - loss: 0.5059 - acc: 0.8300\nEpoch 10/20\n20/20 [==============================] - 1s 54ms/step - loss: 0.4521 - acc: 0.8400\nEpoch 11/20\n20/20 [==============================] - 1s 48ms/step - loss: 0.3747 - acc: 0.8850\nEpoch 12/20\n20/20 [==============================] - 1s 47ms/step - loss: 0.4844 - acc: 0.8475\nEpoch 13/20\n20/20 [==============================] - 1s 51ms/step - loss: 0.3915 - acc: 0.8950\nEpoch 14/20\n20/20 [==============================] - 1s 47ms/step - loss: 0.3431 - acc: 0.8925\nEpoch 15/20\n20/20 [==============================] - 1s 49ms/step - loss: 0.3861 - acc: 0.8825\nEpoch 16/20\n20/20 [==============================] - 1s 59ms/step - loss: 0.3389 - acc: 0.8925\nEpoch 17/20\n20/20 [==============================] - 1s 45ms/step - loss: 0.3340 - acc: 0.8950\nEpoch 18/20\n20/20 [==============================] - 1s 48ms/step - loss: 0.3648 - acc: 0.8825\nEpoch 19/20\n20/20 [==============================] - 1s 52ms/step - loss: 0.3291 - acc: 0.8875\nEpoch 20/20\n20/20 [==============================] - 1s 47ms/step - loss: 0.3388 - acc: 0.9000\n"}],"source":["from tensorflow.keras.callbacks import TensorBoard\n","import numpy as np\n","from datetime import datetime\n","from os import mkdir\n","\n","log_dir = join('logs', 'srp5', datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\"))\n","mkdir(log_dir)\n","\n","from src.training_env import reset\n","reset(log_dir)\n","\n","callbacks = [ TensorBoard(\n","    log_dir=log_dir,\n","    histogram_freq=1,\n","    embeddings_freq=1) ]\n","\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=20,\n","    epochs=20,\n","    callbacks=callbacks)"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["A model evaluation is undertaken to see how well the model performs on data it has never seen before. The first value represents the loss and the second value represents the accuracy."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":"[0.6071663084129493, 0.7866667]"},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["model.evaluate_generator(test_generator)"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["At last the model is saved for later investigation if necessary."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["model_path = join('models', 'srp5.h5')\n","model.save(model_path)"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}